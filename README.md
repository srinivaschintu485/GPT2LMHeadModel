# GPT2LMHeadModel

# Training a Small Language Model in PyTorch

## Overview
This project demonstrates how to train a small transformer-based language model from scratch using PyTorch. It covers everything from tokenization to model architecture, training loop, and text generation. The notebook is structured to be beginner-friendly and educational, with code clarity as a key focus.

## Features
- Lightweight transformer implementation
- Custom tokenizer
- Efficient batching and masking
- Sample text generation
- Entire training process within a single notebook

## Requirements
- Python 3.8+
- Jupyter Notebook
- PyTorch
- NumPy

## How to Use
1. Clone the repository
2. Install required libraries (see requirements section)
3. Launch Jupyter and open `Training-small-Language-Model.ipynb`
4. Run the cells in order to train and test the model

## Sample Output
After training, the model can generate coherent text samples based on learned patterns.

## License
This project is licensed under the MIT License.

## Author
Srinivas Pendotagaya
